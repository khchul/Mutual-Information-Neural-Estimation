{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MINE, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(50,50),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(50,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(dist, N_samples):\n",
    "    minibatch = dist.sample(torch.Size([N_samples]))\n",
    "    marginal = minibatch[torch.randperm(N_samples), 1]\n",
    "\n",
    "    return torch.cat([minibatch, marginal[...,None]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = torch.linspace(-1, 1, 20)[1:-1].to(device)\n",
    "mean_XZ = torch.tensor([0., 0.]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_MI = []\n",
    "naive_MI = []\n",
    "ma_MI = []\n",
    "beta = .01\n",
    "\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rho in corr:\n",
    "    cov_XZ = torch.tensor([[1., rho], [rho, 1.]]).to(device)\n",
    "    XZ = MultivariateNormal(mean_XZ, cov_XZ)\n",
    "    ma = 0\n",
    "\n",
    "    naive_model = MINE().to(device)\n",
    "    ma_model = MINE().to(device)\n",
    "    optimizer_1 = optim.Adam(naive_model.parameters(), lr=1e-4)\n",
    "    optimizer_2 = optim.Adam(ma_model.parameters(), lr=1e-4)\n",
    "\n",
    "    tqdm.write(f'\\n======== Rho = {rho} ========')\n",
    "    tqdm.write(f'GT MI : {-.5 * math.log(1 - rho.item()**2)}')\n",
    "    for e in trange(1, epochs+1):\n",
    "        batch = get_samples(XZ, 500).to(device)\n",
    "        \n",
    "        optimizer_1.zero_grad()\n",
    "        optimizer_2.zero_grad()\n",
    "\n",
    "        # Needs to multiply minus due to pytorch's update method\n",
    "        naive_lbd = -(torch.mean(naive_model(batch[:, :2])) - torch.log(\n",
    "            torch.mean(torch.exp(\n",
    "                naive_model(torch.stack([batch[:, 0], batch[:, 2]], dim=1))\n",
    "                ))\n",
    "            ))  \n",
    "        \n",
    "        et = torch.exp(\n",
    "                ma_model(torch.stack([batch[:, 0], batch[:, 2]], dim=1))\n",
    "                )\n",
    "        if e == 1:\n",
    "            ma = et\n",
    "        else:\n",
    "            ma = beta * ma + (1-beta) * et\n",
    "        \n",
    "        ma_lbd = -(torch.mean(ma_model(batch[:, :2])) - torch.log(\n",
    "            torch.mean(et) * (et / ma).mean().detach()\n",
    "        ))\n",
    "\n",
    "        naive_lbd.backward()\n",
    "        ma_lbd.backward()\n",
    "        optimizer_1.step()\n",
    "        optimizer_2.step()\n",
    "        \n",
    "        if e%250 == 0:\n",
    "            tqdm.write(f'Naive MI : {-naive_lbd}')\n",
    "            tqdm.write(f'Moving Avg MI : {-ma_lbd}')\n",
    "            \n",
    "\n",
    "\n",
    "    test_data = get_samples(XZ, 500).to(device)\n",
    "\n",
    "    gt_MI.append(-.5 * math.log(1 - rho.item()**2))\n",
    "    naive_MI.append((torch.mean(naive_model(test_data[:, :2])) - torch.log(\n",
    "        torch.mean(\n",
    "            torch.exp(\n",
    "                naive_model(torch.stack([batch[:, 0], batch[:, 2]], dim=1))\n",
    "                )\n",
    "            ))\n",
    "        ).item())\n",
    "    ma_MI.append((torch.mean(ma_model(test_data[:, :2])) - torch.log(\n",
    "        torch.mean(\n",
    "            torch.exp(\n",
    "                ma_model(torch.stack([batch[:, 0], batch[:, 2]], dim=1))\n",
    "                )\n",
    "            ))\n",
    "        ).item())\n",
    "\n",
    "\n",
    "    del naive_model, ma_model, batch, test_data\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corr.detach().cpu().numpy()\n",
    "\n",
    "plt.plot(corr, gt_MI,'b',label='Ground Truth')\n",
    "plt.plot(corr, naive_MI,'r',label='Naive MINE')\n",
    "plt.plot(corr, ma_MI,'g',label='EMA MINE')\n",
    "\n",
    "plt.xlabel('Rho')\n",
    "plt.ylabel('Mutual Information')\n",
    "plt.legend(loc='right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
